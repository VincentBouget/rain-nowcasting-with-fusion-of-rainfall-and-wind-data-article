{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging radar rain images and wind predictions in a deep learning model applied to rain nowcasting : Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import samplers\n",
    "import utilities as utils\n",
    "from unet_model import UNet\n",
    "from dataset import MeteoNetDataset\n",
    "from eval import eval_net_and_persistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where rain data is stored\n",
    "rain_dir = r'../meteonet/fulldata/rainmap'                      # <- TO COMPLETE\n",
    "# Directory where wind data are stored\n",
    "U_dir = r'../meteonet/fulldata/wind/U'                            # <- TO COMPLETE\n",
    "V_dir = r'../meteonet/fulldata/wind/V'                            # <- TO COMPLETE\n",
    "# Path to rain radar coordonates file\n",
    "coord_file = r'data/radar_coords_NW.npz'          # <- TO COMPLETE\n",
    "# Boolean to save or not the model at the end of each epoch\n",
    "save_cp = False\n",
    "# Directory to save checkpoints if save_cp==True\n",
    "ckp_dir = r''\n",
    "#Architecture constants\n",
    "Matrix_path = os.path.join(\"PPMatrix\", \"ppMatrix_X0_Y0\")  # Relative path to matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps to use for inputs \n",
    "temporal_length_inputs = 12 # Length input. unit is 5 minutes. 12 = 1h\n",
    "# Number of time steps from the first input to the target\n",
    "temporal_length = 12+6 # Horizon for prediction. Unit is 5 minutes. \n",
    "# Thresholds in mm/h and mm\n",
    "thresholds_in_mmh = [0, 0.1, 1, 2.5]  # CRF over 1h\n",
    "thresholds_in_cent_mm = [100*k/12 for k in thresholds_in_mmh] # CRF over 5 minutes in 1/100 of millimeters like MN data\n",
    "# Input and output channels\n",
    "n_channels = 3*temporal_length_inputs # Factor 3 for rain + U + V\n",
    "n_classes = len(thresholds_in_cent_mm)-1\n",
    "# Proportionnal to the number of weights in the model\n",
    "model_Size = 1 # 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "percentage_sampling = None # see paper section oversampling\n",
    "# Learning rate\n",
    "lr = 0.0008\n",
    "# Cuda, number of loader worker processes.\n",
    "num_workers = 0\n",
    "# Weight decay for L2 regularization\n",
    "wd = 1e-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,\n",
    "          rain_dir,\n",
    "          U_dir,\n",
    "          V_dir,\n",
    "          ckp_dir,\n",
    "          device,\n",
    "          epochs=20,\n",
    "          batch_size=256,\n",
    "          lr=0.0008,\n",
    "          save_cp=True,\n",
    "          num_workers=0,\n",
    "          n_val_round_by_epoch=3,\n",
    "          wd=1e-5,\n",
    "          percentage_sampling=0.8):\n",
    "    \n",
    "    train = MeteoNetDataset(rain_dir=os.path.join(rain_dir,'train'), \n",
    "                            U_dir=os.path.join(U_dir,'train'), \n",
    "                            V_dir=os.path.join(V_dir,'train'),\n",
    "                            temporal_length_inputs=temporal_length_inputs, \n",
    "                            temporal_length=temporal_length, \n",
    "                            temporal_stride=temporal_length_inputs, \n",
    "                            thresholds=thresholds_in_cent_mm, \n",
    "                            Matrix_path=Matrix_path)\n",
    "    val = MeteoNetDataset(rain_dir=os.path.join(rain_dir,'val'), \n",
    "                          U_dir=os.path.join(U_dir,'val'), \n",
    "                          V_dir=os.path.join(V_dir,'val'),\n",
    "                          temporal_length_inputs=temporal_length_inputs, \n",
    "                          temporal_length=temporal_length, \n",
    "                          temporal_stride=temporal_length_inputs, \n",
    "                          thresholds=thresholds_in_cent_mm, \n",
    "                          Matrix_path=Matrix_path)\n",
    "    thresholds_normalized = np.log(np.array(thresholds_in_cent_mm)+1)/train.norm_factor\n",
    "    \n",
    "    train_sampler, real_percentage = samplers.oversample_xpercent_rainy_tiles(train, p=percentage_sampling, \n",
    "                                                                              above_class=len(thresholds_in_cent_mm)-1)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers, pin_memory=True)\n",
    "    # Custom sampler to consider only defined data\n",
    "    val_sampler = samplers.CustomSampler(samplers.indices_except_undefined_sampler(val))\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    writer = SummaryWriter(comment=f'-LR_{lr}_BS_{batch_size}_E_{epochs}_T{temporal_length}_WD_{wd}_p{percentage_sampling}')\n",
    "    writer.add_scalar('Number_of_parameters', net.get_nb_params())\n",
    "    info = f'''Starting training:\n",
    "        Epochs:                {epochs}\n",
    "        Learning rate:         {lr}\n",
    "        Batch size:            {batch_size}\n",
    "        Weight decay:          {wd}\n",
    "        Number batch train :   {len(train_loader)}\n",
    "        Number batch val :     {len(val_loader)}\n",
    "        Training size:         {len(train)}\n",
    "        Validation size:       {len(val)}\n",
    "        Checkpoints:           {save_cp}\n",
    "        Device:                {device.type}\n",
    "        Number of parameters:  {net.get_nb_params()}\n",
    "        Thresholds(mm/h):      {thresholds_in_mmh}\n",
    "        Temporal length:       {temporal_length}\n",
    "        Normalization factor:  {train.norm_factor}\n",
    "        Percentage sampling:   {percentage_sampling}\n",
    "        Real percantage :      {real_percentage}\n",
    "    '''\n",
    "    logging.info(info)\n",
    "    writer.add_text('Description', info)\n",
    "\n",
    "    global_step = 0\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        n_batch = len(train_loader)\n",
    "        \n",
    "        with tqdm(total=n_batch, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:\n",
    "            for batch in train_loader:\n",
    "                imgs = batch['inputs']  #BC(temp)HW\n",
    "                true_imgs = batch['target']   #BClsHW\n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "                   \n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                true_imgs = true_imgs.to(device=device, dtype=torch.float32)\n",
    "                \n",
    "                imgs_pred = net(imgs)   #BClsHW\n",
    "                loss = criterion(imgs_pred, true_imgs)\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix(**{'loss (epoch)': epoch_loss})\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "                pbar.update()\n",
    "                \n",
    "                if ((global_step % (n_batch//n_val_round_by_epoch))==0):\n",
    "                    Scores = eval_net_and_persistance(net=net, thresholds=thresholds_normalized, loader=val_loader, device=device)\n",
    "                    # Log scores\n",
    "                    for key in Scores.keys():\n",
    "                        score = Scores[key]\n",
    "                        if type(score)==type(dict()):\n",
    "                            writer.add_scalars(f'{key}_val', score, global_step)\n",
    "                        else :\n",
    "                            writer.add_scalar(f'{key}_val', score, global_step)\n",
    "                            \n",
    "                    # Save images of batch inputs//target, prediction, persistance, (pred-target) and (pers-target) on the last epoch\n",
    "                    if (epoch==epochs-1):    \n",
    "                        # we select rain, U and V channels (BCHW)\n",
    "                        utils.writer_add_batch_rain(rain_channels=imgs[:,:12,:,:],text=\"Rain_inputs\",writer=writer)\n",
    "                        utils.writer_add_batch_wind(wind_channels=imgs[:,12:24,:,:],text=\"U_inputs\",writer=writer,isU=True)\n",
    "                        utils.writer_add_batch_wind(wind_channels=imgs[:,24:,:,:],text=\"V_inputs\",writer=writer,isU=False)\n",
    "                        utils.writer_add_comparison(imgs,true_imgs,imgs_pred,\n",
    "                                                    text=\"Target_Prediction_Persistance_Prediction-Target_Persistance-Target\",\n",
    "                                                    writer=writer,thresholds=thresholds_normalized, temporal_length_inputs=temporal_length_inputs)\n",
    "        \n",
    "                global_step += 1\n",
    "        losses = {\"lossTot\" : epoch_loss}\n",
    "        writer.add_scalars('Loss_train', losses, epoch)\n",
    "        \n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(ckp_dir)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       os.path.join(ckp_dir, f'CP_epoch{epoch + 1}.pth'))\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved !')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cuda\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Precipitation matrix path specified does not exist : ../meteonet/fulldata/wind/U/train/PPMatrix/ppMatrix_X0_Y0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# summary(net)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrain_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mU_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mU_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m          \u001b[49m\u001b[43mV_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m          \u001b[49m\u001b[43mckp_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m          \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m          \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m          \u001b[49m\u001b[43mpercentage_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpercentage_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m          \u001b[49m\u001b[43msave_cp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_cp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, rain_dir, U_dir, V_dir, ckp_dir, device, epochs, batch_size, lr, save_cp, num_workers, n_val_round_by_epoch, wd, percentage_sampling)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(net,\n\u001b[1;32m      2\u001b[0m           rain_dir,\n\u001b[1;32m      3\u001b[0m           U_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m           wd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[1;32m     14\u001b[0m           percentage_sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mMeteoNetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mU_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mV_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtemporal_length_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporal_length_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtemporal_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporal_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtemporal_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporal_length_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthresholds_in_cent_mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mMatrix_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMatrix_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     val \u001b[38;5;241m=\u001b[39m MeteoNetDataset(rain_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(rain_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[1;32m     25\u001b[0m                           U_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(U_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[1;32m     26\u001b[0m                           V_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(V_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m                           thresholds\u001b[38;5;241m=\u001b[39mthresholds_in_cent_mm, \n\u001b[1;32m     31\u001b[0m                           Matrix_path\u001b[38;5;241m=\u001b[39mMatrix_path)\n\u001b[1;32m     32\u001b[0m     thresholds_normalized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39marray(thresholds_in_cent_mm)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39mtrain\u001b[38;5;241m.\u001b[39mnorm_factor\n",
      "File \u001b[0;32m/local/bereziat/rain-nowcasting-with-fusion-of-rainfall-and-wind-data-article/dataset.py:44\u001b[0m, in \u001b[0;36mMeteoNetDataset.__init__\u001b[0;34m(self, rain_dir, U_dir, V_dir, temporal_length_inputs, temporal_length, temporal_stride, thresholds, Matrix_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPPMatrix), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecipitation matrix path specified does not exist : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPPMatrix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_Matrix \u001b[38;5;241m=\u001b[39m  os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(U_dir, Matrix_path) \n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_Matrix), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecipitation matrix path specified does not exist : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_Matrix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV_Matrix \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(V_dir, Matrix_path)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV_Matrix), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecipitation matrix path specified does not exist : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV_Matrix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Precipitation matrix path specified does not exist : ../meteonet/fulldata/wind/U/train/PPMatrix/ppMatrix_X0_Y0"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "    \n",
    "    net = UNet(n_channels, n_classes, bilinear=True, n=model_Size)\n",
    "    net.to(device=device)\n",
    "    # summary(net)\n",
    "    try:\n",
    "        train(net=net,\n",
    "              rain_dir=rain_dir,\n",
    "              U_dir=U_dir,\n",
    "              V_dir=V_dir,\n",
    "              ckp_dir=ckp_dir,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              lr=lr,\n",
    "              device=device,\n",
    "              num_workers=num_workers,\n",
    "              wd = wd,\n",
    "              percentage_sampling=percentage_sampling,\n",
    "              save_cp=save_cp)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "logs_dir = './runs'\n",
    "%tensorboard --logdir {logs_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4127090633.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [10]\u001b[0;36m\u001b[0m\n\u001b[0;31m    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from dataset import DatasetPrediction\n",
    "directory = os.getcwd()\n",
    "\n",
    "# Path to model, specify to load a model, otherwise let empty string\n",
    "model = r'weigths/model30m.pth'\n",
    "model = os.path.join(directory,model)\n",
    "assert os.path.isfile(model), 'weight file not found'\n",
    "rain_dir = r'data/pred_example/rain'\n",
    "rain_dir = os.path.join(directory,rain_dir)\n",
    "assert os.path.isdir(rain_dir), 'rain file not found'\n",
    "wind_dir = r'data/pred_example'\n",
    "wind_dir = os.path.join(directory,wind_dir)\n",
    "assert os.path.isdir(wind_dir), 'wind file not found'\n",
    "\n",
    "if __name__ == '__main__':cs_l\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "    \n",
    "    net = UNet(n_channels, n_classes, bilinear=True, n=8)\n",
    "    net.load_state_dict(\n",
    "        torch.load(model, map_location=device)\n",
    "    )\n",
    "    logging.info(f'Model loaded from {model}')\n",
    "    net.to(device=device)\n",
    "    net.eval()\n",
    "    \n",
    "    dataset = DatasetPrediction(rain_dir=rain_dir, \n",
    "                            U_dir=os.path.join(wind_dir,'U'), \n",
    "                            V_dir=os.path.join(wind_dir,'V'),\n",
    "                            thresholds=thresholds_in_cent_mm)\n",
    "    thresholds_normalized = np.log(np.array(thresholds_in_cent_mm)+1)/dataset.norm_factor\n",
    "    dataset_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    batch = next(iter(dataset_loader))\n",
    "    imgs, true_imgs = batch['inputs'], batch['target']\n",
    "    imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "    true_imgs = true_imgs.to(device=device, dtype=torch.float32)\n",
    "    persistance = utils.batch_to_mapped_persistance(imgs,thresholds_normalized)\n",
    "    # Network output\n",
    "    with torch.no_grad():\n",
    "        imgs_pred = net(imgs)\n",
    "        \n",
    "    utils.plot_comparison(imgs,true_imgs,imgs_pred,thresholds_normalized,temporal_length_inputs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
