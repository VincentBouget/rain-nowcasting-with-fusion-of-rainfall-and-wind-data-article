{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging radar rain images and wind predictions in a deep learning model applied to rain nowcasting : Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import samplers\n",
    "import utilities as utils\n",
    "from unet_model import UNet\n",
    "from dataset import MeteoNetDataset\n",
    "from eval import eval_net_and_persistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where rain data is stored\n",
    "rain_dir = r'data/Rain'                      # <- TO COMPLETE\n",
    "# Directory where wind data are stored\n",
    "U_dir = r'data/U'                            # <- TO COMPLETE\n",
    "V_dir = r'data/V'                            # <- TO COMPLETE\n",
    "# Path to rain radar coordonates file\n",
    "coord_file = r'data/radar_coords_NW.npz'          # <- TO COMPLETE\n",
    "# Boolean to save or not the model at the end of each epoch\n",
    "save_cp = True\n",
    "# Directory to save checkpoints if save_cp==True\n",
    "ckp_dir = r'runs'\n",
    "#Architecture constants\n",
    "Matrix_path = os.path.join(\"Matrix\", \"Matrix\")  # Relative path to matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps to use for inputs \n",
    "temporal_length_inputs = 12 #1h input\n",
    "# Number of time steps from the first input to the target\n",
    "temporal_length = temporal_length_inputs+6 # Prediction at 30m. Set 24 for 1h.\n",
    "# Thresholds in mm/h and mm\n",
    "thresholds_in_mmh = [0, 0.1, 1, 2.5]  # CRF over 1h\n",
    "thresholds_in_cent_mm = [100*k/12 for k in thresholds_in_mmh] # CRF over 5 minutes in 1/100 of millimeters like MN data\n",
    "# Input and output channels\n",
    "n_channels = 3*temporal_length_inputs # Factor 3 for rain + U + V\n",
    "n_classes = len(thresholds_in_cent_mm)-1\n",
    "# Proportionnal to the number of weights in the model\n",
    "model_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "percentage_sampling = 0.9 # see paper section oversampling\n",
    "# Learning rate\n",
    "lr = {0:0.0008,4:0.0001}\n",
    "# Cuda, number of loader worker processes.\n",
    "num_workers = 0\n",
    "# Weight decay for L2 regularization\n",
    "wd = {0:1e-5 ,4:5e-5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,\n",
    "          rain_dir,\n",
    "          U_dir,\n",
    "          V_dir,\n",
    "          ckp_dir,\n",
    "          device,\n",
    "          epochs=20,\n",
    "          batch_size=256,\n",
    "          lr={0:1e-4},\n",
    "          save_cp=True,\n",
    "          num_workers=0,\n",
    "          n_val_round_by_epoch=3,\n",
    "          wd={0:1e-5},\n",
    "          percentage_sampling=0.8):\n",
    "    \n",
    "    train = MeteoNetDataset(rain_dir=os.path.join(rain_dir,'train'), \n",
    "                            U_dir=os.path.join(U_dir,'train'), \n",
    "                            V_dir=os.path.join(V_dir,'train'),\n",
    "                            temporal_length_inputs=temporal_length_inputs, \n",
    "                            temporal_length=temporal_length, \n",
    "                            temporal_stride=temporal_length_inputs, \n",
    "                            thresholds=thresholds_in_cent_mm, \n",
    "                            Matrix_path=Matrix_path)\n",
    "    val = MeteoNetDataset(rain_dir=os.path.join(rain_dir,'val'), \n",
    "                          U_dir=os.path.join(U_dir,'val'), \n",
    "                          V_dir=os.path.join(V_dir,'val'),\n",
    "                          temporal_length_inputs=temporal_length_inputs, \n",
    "                          temporal_length=temporal_length, \n",
    "                          temporal_stride=temporal_length_inputs, \n",
    "                          thresholds=thresholds_in_cent_mm, \n",
    "                          Matrix_path=Matrix_path)\n",
    "    thresholds_normalized = np.log(np.array(thresholds_in_cent_mm)+1)/train.norm_factor\n",
    "    \n",
    "    train_sampler, real_percentage = samplers.oversample_xpercent_rainy_tiles(train, p=percentage_sampling, \n",
    "                                                                              above_class=len(thresholds_in_cent_mm)-1)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers, pin_memory=True)\n",
    "    # Custom sampler to consider only defined data\n",
    "    val_sampler = samplers.CustomSampler(samplers.indices_except_undefined_sampler(val))\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    writer = SummaryWriter(comment=f'-LR_{lr}_BS_{batch_size}_E_{epochs}_T{temporal_length}_WD_{wd}_p{percentage_sampling}')\n",
    "    writer.add_scalar('Number_of_parameters', net.get_nb_params())\n",
    "    info = f'''Starting training:\n",
    "        Epochs:                {epochs}\n",
    "        Learning rate:         {lr}\n",
    "        Batch size:            {batch_size}\n",
    "        Weight decay:          {wd}\n",
    "        Number batch train :   {len(train_loader)}\n",
    "        Number batch val :     {len(val_loader)}\n",
    "        Training size:         {len(train)}\n",
    "        Validation size:       {len(val)}\n",
    "        Checkpoints:           {save_cp}\n",
    "        Device:                {device.type}\n",
    "        Number of parameters:  {net.get_nb_params()}\n",
    "        Thresholds(mm/h):      {thresholds_in_mmh}\n",
    "        Temporal length:       {temporal_length}\n",
    "        Normalization factor:  {train.norm_factor}\n",
    "        Percentage sampling:   {percentage_sampling}\n",
    "        Real percantage :      {real_percentage}\n",
    "    '''\n",
    "    logging.info(info)\n",
    "    writer.add_text('Description', info)\n",
    "\n",
    "    global_step = 0\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if epoch in lr.keys():\n",
    "            print('***:',lr[epoch], wd[epoch])\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr[epoch], weight_decay=wd[epoch])\n",
    "            \n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        n_batch = len(train_loader)\n",
    "        \n",
    "        with tqdm(total=n_batch, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:\n",
    "            for batch in train_loader:\n",
    "                imgs = batch['inputs']  #BC(temp)HW\n",
    "                true_imgs = batch['target']   #BClsHW\n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "                   \n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                true_imgs = true_imgs.to(device=device, dtype=torch.float32)\n",
    "                \n",
    "                imgs_pred = net(imgs)   #BClsHW\n",
    "                loss = criterion(imgs_pred, true_imgs)\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix(**{'loss (epoch)': epoch_loss})\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "                pbar.update()\n",
    "                \n",
    "                if ((global_step % (n_batch//n_val_round_by_epoch))==0):\n",
    "                    Scores = eval_net_and_persistance(net=net, thresholds=thresholds_normalized, loader=val_loader, device=device)\n",
    "                    # Log scores\n",
    "                    for key in Scores.keys():\n",
    "                        score = Scores[key]\n",
    "                        if type(score)==type(dict()):\n",
    "                            writer.add_scalars(f'{key}_val', score, global_step)\n",
    "                        else :\n",
    "                            writer.add_scalar(f'{key}_val', score, global_step)\n",
    "                            \n",
    "                    # Save images of batch inputs//target, prediction, persistance, (pred-target) and (pers-target) on the last epoch\n",
    "                    if (epoch==epochs-1):    \n",
    "                        # we select rain, U and V channels (BCHW)\n",
    "                        utils.writer_add_batch_rain(rain_channels=imgs[:,:12,:,:],text=\"Rain_inputs\",writer=writer)\n",
    "                        utils.writer_add_batch_wind(wind_channels=imgs[:,12:24,:,:],text=\"U_inputs\",writer=writer,isU=True)\n",
    "                        utils.writer_add_batch_wind(wind_channels=imgs[:,24:,:,:],text=\"V_inputs\",writer=writer,isU=False)\n",
    "                        utils.writer_add_comparison(imgs,true_imgs,imgs_pred,\n",
    "                                                    text=\"Target_Prediction_Persistance_Prediction-Target_Persistance-Target\",\n",
    "                                                    writer=writer,thresholds=thresholds_normalized, temporal_length_inputs=temporal_length_inputs)\n",
    "        \n",
    "                global_step += 1\n",
    "        losses = {\"lossTot\" : epoch_loss}\n",
    "        writer.add_scalars('Loss_train', losses, epoch)\n",
    "        \n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(ckp_dir)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       os.path.join(ckp_dir, f'CP_epoch{epoch + 1}.pth'))\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved !')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "    \n",
    "    net = UNet(n_channels, n_classes, bilinear=True, n=model_size)\n",
    "    net.to(device=device)\n",
    "    \n",
    "    try:\n",
    "        train(net=net,\n",
    "              rain_dir=rain_dir,\n",
    "              U_dir=U_dir,\n",
    "              V_dir=V_dir,\n",
    "              ckp_dir=ckp_dir,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              lr=lr,\n",
    "              device=device,\n",
    "              num_workers=num_workers,\n",
    "              wd = wd,\n",
    "              percentage_sampling=percentage_sampling,\n",
    "              save_cp=save_cp)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "logs_dir = './runs'\n",
    "%tensorboard --logdir {logs_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dataset import DatasetPrediction\n",
    "directory = os.getcwd()\n",
    "\n",
    "# Path to model, specify to load a model, otherwise let empty string\n",
    "# model = r'weigths/model30m.pth'\n",
    "model = 'runs/CP_epoch20.pth'\n",
    "model = os.path.join(directory,model)\n",
    "assert os.path.isfile(model), 'weight file not found'\n",
    "rain_dir = r'data/pred_example/rain'\n",
    "rain_dir = os.path.join(directory,rain_dir)\n",
    "assert os.path.isdir(rain_dir), 'rain file not found'\n",
    "wind_dir = r'data/pred_example'\n",
    "wind_dir = os.path.join(directory,wind_dir)\n",
    "assert os.path.isdir(wind_dir), 'wind file not found'\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "    \n",
    "    net = UNet(n_channels, n_classes, bilinear=True, n=model_size)\n",
    "    net.load_state_dict(\n",
    "        torch.load(model, map_location=device)\n",
    "    )\n",
    "    logging.info(f'Model loaded from {model}')\n",
    "    net.to(device=device)\n",
    "    net.eval()\n",
    "    \n",
    "    dataset = DatasetPrediction(rain_dir=rain_dir, \n",
    "                            U_dir=os.path.join(wind_dir,'U'), \n",
    "                            V_dir=os.path.join(wind_dir,'V'),\n",
    "                            thresholds=thresholds_in_cent_mm)\n",
    "    thresholds_normalized = np.log(np.array(thresholds_in_cent_mm)+1)/dataset.norm_factor\n",
    "    dataset_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    batch = next(iter(dataset_loader))\n",
    "    imgs, true_imgs = batch['inputs'], batch['target']\n",
    "    imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "    true_imgs = true_imgs.to(device=device, dtype=torch.float32)\n",
    "    persistance = utils.batch_to_mapped_persistance(imgs,thresholds_normalized)\n",
    "    # Network output\n",
    "    with torch.no_grad():\n",
    "        imgs_pred = net(imgs)\n",
    "        \n",
    "    utils.plot_comparison(imgs,true_imgs,imgs_pred,thresholds_normalized,temporal_length_inputs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
